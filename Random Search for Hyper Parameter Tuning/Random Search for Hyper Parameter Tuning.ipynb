{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e45aaf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1 align=\"center\">Random Search for Hyper Parameter Tuning</h1>\n",
    "    <h3 align=\"center\"> Tabular Time Series</h3>\n",
    "    <h5 align=\"center\">Github: (https://github.com/MTisMT)</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a85ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import spearmanr\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn\")\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    " \n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd81b9",
   "metadata": {},
   "source": [
    "# Read data\n",
    "A financial market dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac51cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"X_train.csv\")\n",
    "y = pd.read_csv(\"y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e582849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>Feature_11</th>\n",
       "      <th>Feature_12</th>\n",
       "      <th>Feature_13</th>\n",
       "      <th>Feature_14</th>\n",
       "      <th>Feature_15</th>\n",
       "      <th>Feature_16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "      <td>16651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.489805</td>\n",
       "      <td>0.255825</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.255750</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.499910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.357614</td>\n",
       "      <td>0.282637</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.283202</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.353787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Feature_1     Feature_2     Feature_3     Feature_4     Feature_5  \\\n",
       "count  16651.000000  16651.000000  16651.000000  16651.000000  16651.000000   \n",
       "mean       0.489805      0.255825      0.499910      0.499910      0.499910   \n",
       "std        0.357614      0.282637      0.353787      0.353787      0.353787   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.250000      0.000000      0.250000      0.250000      0.250000   \n",
       "50%        0.500000      0.250000      0.500000      0.500000      0.500000   \n",
       "75%        0.750000      0.500000      0.750000      0.750000      0.750000   \n",
       "max        1.000000      0.750000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          Feature_6     Feature_7     Feature_8     Feature_9    Feature_10  \\\n",
       "count  16651.000000  16651.000000  16651.000000  16651.000000  16651.000000   \n",
       "mean       0.499910      0.255750      0.499910      0.499910      0.499910   \n",
       "std        0.353787      0.283202      0.353787      0.353787      0.353787   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.250000      0.000000      0.250000      0.250000      0.250000   \n",
       "50%        0.500000      0.250000      0.500000      0.500000      0.500000   \n",
       "75%        0.750000      0.500000      0.750000      0.750000      0.750000   \n",
       "max        1.000000      0.750000      1.000000      1.000000      1.000000   \n",
       "\n",
       "         Feature_11    Feature_12    Feature_13    Feature_14    Feature_15  \\\n",
       "count  16651.000000  16651.000000  16651.000000  16651.000000  16651.000000   \n",
       "mean       0.499910      0.499910      0.499910      0.499910      0.499910   \n",
       "std        0.353787      0.353787      0.353787      0.353787      0.353787   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.250000      0.250000      0.250000      0.250000      0.250000   \n",
       "50%        0.500000      0.500000      0.500000      0.500000      0.500000   \n",
       "75%        0.750000      0.750000      0.750000      0.750000      0.750000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "         Feature_16  \n",
       "count  16651.000000  \n",
       "mean       0.499910  \n",
       "std        0.353787  \n",
       "min        0.000000  \n",
       "25%        0.250000  \n",
       "50%        0.500000  \n",
       "75%        0.750000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87c56f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.499910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.353787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  y\n",
       "count  16651.000000\n",
       "mean       0.499910\n",
       "std        0.353787\n",
       "min        0.000000\n",
       "25%        0.250000\n",
       "50%        0.500000\n",
       "75%        0.750000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76740631",
   "metadata": {},
   "source": [
    "# Time series cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f473c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_crossval_block(X,y,k = 5, test_percent = 0.2):\n",
    "    wsize = int(X.shape[0]//k)\n",
    "    print(\"Each fold size: \", wsize)\n",
    "    test_part = int(test_percent * wsize)\n",
    "    train_lb = 0\n",
    "    train_ub = train_lb + wsize\n",
    "    X_train={}; y_train={}\n",
    "    X_test={}; y_test={}\n",
    "    \n",
    "    fig = plt.figure(figsize = (12,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    for i in range(1,k+1):\n",
    "        \n",
    "        train_ub = int((1-test_percent)* wsize) +train_lb\n",
    "        test_part = int(test_percent * wsize)\n",
    "#         train_ub = train_lb + wsize\n",
    "        print(\"fold\",i,\"===> train: \",train_lb,\"_\",train_ub,\" | test: \",train_ub,\"_\",train_ub + test_part)\n",
    "        print(f\"Train Percentage: {(train_ub-train_lb) /(train_ub + test_part-train_lb):.0%}\")\n",
    "        print(\"_\"*80)\n",
    "        X_train[i] = X[train_lb:train_ub]; y_train[i] = y[train_lb:train_ub]\n",
    "        X_test[i] = X[train_ub:train_ub +test_part]; y_test[i] = y[train_ub:train_ub + test_part]\n",
    "        \n",
    "        rect1 = matplotlib.patches.Rectangle(((train_lb/X.shape[0])*100,2*i*10), ((train_ub-train_lb)/X.shape[0])*100, 10, color='blue')\n",
    "        rect2 = matplotlib.patches.Rectangle(((train_ub/X.shape[0])*100, 2*i*10),((test_part)/X.shape[0])*100 , 10, color='k')\n",
    "        ax.add_patch(rect1)\n",
    "        ax.add_patch(rect2)\n",
    "        \n",
    "        train_lb += wsize\n",
    "        \n",
    "    print(\"\\n\") \n",
    "    plt.xlim([0, 100])\n",
    "    plt.ylim([0, 2*10*(k+1)])\n",
    "    plt.show()\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06bfbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_crossval(X,y,k = 5, test_percent = 0.2):\n",
    "    wsize = int(X.shape[0]//k)\n",
    "    test_part = int(test_percent * wsize)\n",
    "    train_lb = 0\n",
    "#     train_ub = train_lb + wsize\n",
    "    X_train={}; y_train={}\n",
    "    X_test={}; y_test={}\n",
    "    \n",
    "    fig = plt.figure(figsize = (12,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    for i in range(1,k+1):\n",
    "        \n",
    "        train_ub = int((1-test_percent)* (train_lb + wsize))\n",
    "        test_part = int(test_percent *(train_lb + wsize))\n",
    "        print(\"fold\",i,\"===> train: \",0,\"_\",train_ub,\" | test: \",train_ub,\"_\",train_ub + test_part)\n",
    "        print(f\"Train Percentage: {train_ub /(train_ub + test_part):.0%}\")\n",
    "        print(\"_\"*80)\n",
    "        X_train[i] = X[0:train_ub]; y_train[i] = y[0:train_ub]\n",
    "        X_test[i] = X[train_ub:train_ub + test_part]; y_test[i] = y[train_ub:train_ub + test_part]\n",
    "        train_lb += wsize\n",
    "        \n",
    "        rect1 = matplotlib.patches.Rectangle((0,2*i*10), (train_ub/X.shape[0])*100, 10, color='blue')\n",
    "        rect2 = matplotlib.patches.Rectangle(((train_ub/X.shape[0])*100, 2*i*10),((test_part)/X.shape[0])*100 , 10, color='k')\n",
    "        ax.add_patch(rect1)\n",
    "        ax.add_patch(rect2)\n",
    "    print(\"\\n\") \n",
    "    plt.xlim([0, 100])\n",
    "    plt.ylim([0, 2*10*(k+1)])\n",
    "    plt.show()\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ab1fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 ===> train:  0 _ 2664  | test:  2664 _ 3330\n",
      "Train Percentage: 80%\n",
      "________________________________________________________________________________\n",
      "fold 2 ===> train:  0 _ 5328  | test:  5328 _ 6660\n",
      "Train Percentage: 80%\n",
      "________________________________________________________________________________\n",
      "fold 3 ===> train:  0 _ 7992  | test:  7992 _ 9990\n",
      "Train Percentage: 80%\n",
      "________________________________________________________________________________\n",
      "fold 4 ===> train:  0 _ 10656  | test:  10656 _ 13320\n",
      "Train Percentage: 80%\n",
      "________________________________________________________________________________\n",
      "fold 5 ===> train:  0 _ 13320  | test:  13320 _ 16650\n",
      "Train Percentage: 80%\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAFoCAYAAABDtK1/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWaUlEQVR4nO3dUWzV9f3/8dfhNOAsrUhSExPDYtE6iXFzsqKZMr0w1QsvZsyUJtXNLZmETOucgTABF3GMsRAXkk0lLuaHIpJoMi+2m6GROUxjnNNItCRkMZHBVoX81jYGDuX8L/6x+02Pn2opPQiPx5WH0/P9vtV3yvN8+01PpV6v1wMAADQ0o9kDAADAyUwwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUfKZgfuONN9LX15ckefvtt9Pb25u+vr58//vfz/vvv58k2b59e2666aZ85zvfyYsvvnjiJgYAgGnUMtEXbN68Oc8//3y+9KUvJUkeeuihrFq1KhdffHG2bduWzZs35wc/+EG2bNmSZ599NocPH05vb2+++c1vZubMmSf8XwAAAE6kCa8wz5s3L5s2bRp/vHHjxlx88cVJkrGxscyaNStvvvlmLrvsssycOTNtbW2ZN29e3nnnnRM3NQAATJMJg7mnpyctLf+5EH3OOeckSf7617/mySefzHe/+92MjIykra1t/GtaW1szMjJyAsYFAIDpNeEtGY384Q9/yG9/+9s89thjmTt3bmbPnp3R0dHx50dHR/8roD9NvV5PpVKZzAgAADAtPncw//73v88zzzyTLVu2ZM6cOUmSSy+9NA8//HAOHz6cI0eOZO/evenq6prwWJVKJUNDw597aE5tHR1t9oJPsBc0Yi9oxF7QSEfHxBdzP83nCuaxsbE89NBDOffcc/OjH/0oSfKNb3wjd911V/r6+tLb25t6vZ577rkns2bNmvRQAABwsqjU6/V6MwfwDpCPc2WARuwFjdgLGrEXNHI8V5h9cAkAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgIKWZp58z57k4MFKM0fgJHTokL3gk+wFjdgLPm5sbCzvv78n//u/I80ehZNMR8fXJ/3aSr1er0/hLJ/v5L7HAQBTak+Si5o9BCeh40let2QAAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUPCZgvmNN95IX19fkuTdd9/NkiVL0tvbmzVr1uTYsWNJku3bt+emm27Kd77znbz44osnbmIAAJhGEwbz5s2bc//99+fw4cNJknXr1qW/vz9bt25NvV7Pjh07MjQ0lC1btmTbtm15/PHHs3Hjxhw5cuSEDw8AACfahME8b968bNq0afzx7t27093dnSRZvHhxdu3alTfffDOXXXZZZs6cmba2tsybNy/vvPPOiZsaAACmSctEX9DT05P33ntv/HG9Xk+lUkmStLa2Znh4OCMjI2lraxv/mtbW1oyMjEx48sHByYwMANDY2Nj8JIOpVps9CaeSCYP542bM+M9F6dHR0bS3t2f27NkZHR39rz//vwH9abq6kqGh4c87Aqe4jo42e8En2AsasRc00tHRZS+YUp/7t2QsWLAgAwMDSZKdO3dm4cKFufTSS/Paa6/l8OHDGR4ezt69e9PV1TXlwwIAwHT73FeYly9fnlWrVmXjxo3p7OxMT09PqtVq+vr60tvbm3q9nnvuuSezZs06EfMCAMC0qtTr9XozB/AjEz7Oj1hpxF7QiL2gEXtBIx0dE98u/Gl8cAkAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAEBBSzNPvmdPcvBgpZkjcBI6dMhe8En2gkamay/GxsaS7E21esJPxRSYO/erzR6BU0ylXq/Xm3Zyf/cB8IWwJ8lFzR6Cz2hwcDBnn31us8fgJNPR0Tbp17olAwAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAICClsm8qFarZcWKFdm3b19mzJiRBx98MC0tLVmxYkUqlUouvPDCrFmzJjNm6HEAAL7YJhXML730Uo4ePZpt27blL3/5Sx5++OHUarX09/dn0aJFWb16dXbs2JHrrrtuqucFAIBpNalLwOeff37GxsZy7NixjIyMpKWlJbt37053d3eSZPHixdm1a9eUDgoAAM0wqSvMZ555Zvbt25cbbrghhw4dyiOPPJJXX301lUolSdLa2prh4eEJjzM4OJmzA8D0Ghubn2Qw1WqzJ+GzmD9/fqr+ZzGFJhXMTzzxRK666qrce++92b9/f26//fbUarXx50dHR9Pe3j7hcbq6kqGhicOa00tHR5u94BPsBY1M716cO03n4XhVq1XfL/iEjo62Sb92UrdktLe3p63t/5/0rLPOytGjR7NgwYIMDAwkSXbu3JmFCxdOeigAADhZVOr1ev3zvmh0dDQrV67M0NBQarVabrvttlxyySVZtWpVarVaOjs7s3bt2s/04xDvAPk4VxJpxF7QiL2gEXtBI8dzhXlSwTyVLDQf5xsdjdgLGrEXNGIvaGTab8kAAIDThWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoaGnmyffsSQ4erDRzBE5Chw7ZCz7pVNuLsbGxJHtTrTZ7ki+2uXO/2uwRgNNAU4P5oouSZHYzR+CkZS9o5FTaiz1JLm/2EF94g4ODOfvsc5s9BnCKc0sGAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAICClsm+8NFHH80LL7yQWq2WJUuWpLu7OytWrEilUsmFF16YNWvWZMYMPQ4AwBfbpIp2YGAgr7/+ep5++uls2bIlBw4cyLp169Lf35+tW7emXq9nx44dUz0rAABMu0kF88svv5yurq4sW7Ysd955Z6655prs3r073d3dSZLFixdn165dUzooAAA0w6RuyTh06FD+8Y9/5JFHHsl7772XpUuXpl6vp1KpJElaW1szPDw8pYMCAEAzTCqY58yZk87OzsycOTOdnZ2ZNWtWDhw4MP786Oho2tvbJzzO4OBkzg7wxTc2Nj/JYKrVZk/yxTZ//vxU/UekgY6OtmaPwClkUsF8+eWX53/+53/yve99L//617/y4Ycf5sorr8zAwEAWLVqUnTt35oorrpjwOF1dydCQK9H8t46ONnvBJ5yae3Fuswf4wqtWq6fgXnC8Ts3vFxyv43kTNalgvvbaa/Pqq6/m5ptvTr1ez+rVq3Peeedl1apV2bhxYzo7O9PT0zPpoQAA4GRRqdfr9WYO4B0gH+fKAI3YCxqxFzRiL2jkeK4w+0XJAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKWpp58j17koMHK80cgZPQoUP2YrLGxsaS7E212uxJpt7cuV9t9ggAnKaaGswXXZQks5s5AictezE5e5Jc3uwhTojBwcGcffa5zR4DgNOQWzIAAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKDiuYP7ggw/yrW99K3v37s27776bJUuWpLe3N2vWrMmxY8emakYAAGiaSQdzrVbL6tWrc8YZZyRJ1q1bl/7+/mzdujX1ej07duyYsiEBAKBZJh3M69evz6233ppzzjknSbJ79+50d3cnSRYvXpxdu3ZNzYQAANBELZN50XPPPZe5c+fm6quvzmOPPZYkqdfrqVQqSZLW1tYMDw9PeJzBwcmcHfg0Y2PzkwymWm32JFNv/vz5qZ6K/2Ict46OtmaPwEnIXjCVJhXMzz77bCqVSl555ZW8/fbbWb58eQ4ePDj+/OjoaNrb2yc8TldXMjQ0cVhzeunoaLMXx+XcZg9wQlSrVXvBJ/h+QSP2gkaO503UpIL5qaeeGv/nvr6+PPDAA9mwYUMGBgayaNGi7Ny5M1dcccWkhwIAgJPFlP1aueXLl2fTpk255ZZbUqvV0tPTM1WHBgCApqnU6/V6MwfwIxM+zo/SaMRe0Ii9oBF7QSPHc0uGDy4BAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoaGnmyffsSQ4erDRzBD6DsbGxJHtTrU7P+ebO/er0nAgA4DNoajBfdFGSzG7mCHwme5JcPm1nGxwczNlnnztt5wMAKHFLBgAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFLZN5Ua1Wy8qVK7Nv374cOXIkS5cuzQUXXJAVK1akUqnkwgsvzJo1azJjhh4HAOCLbVLB/Pzzz2fOnDnZsGFDDh06lG9/+9v5yle+kv7+/ixatCirV6/Ojh07ct111031vAAAMK0mdQn4+uuvz9133z3+uFqtZvfu3enu7k6SLF68OLt27ZqaCQEAoIkmdYW5tbU1STIyMpK77ror/f39Wb9+fSqVyvjzw8PDEx5ncHAyZ2e6jY3NTzKYanV6zjd//vxUp+tkfKF0dLQ1ewROQvaCRuwFU2lSwZwk+/fvz7Jly9Lb25sbb7wxGzZsGH9udHQ07e3tEx6jqysZGpo4rDkZnDttZ6pWq/aCT+joaLMXfIK9oBF7QSPH8yZqUrdkvP/++7njjjty33335eabb06SLFiwIAMDA0mSnTt3ZuHChZMeCgAAThaTCuZHHnkk//73v/Ob3/wmfX196evrS39/fzZt2pRbbrkltVotPT09Uz0rAABMu0q9Xq83cwA/MuHj/CiNRuwFjdgLGrEXNDLtt2QAAMDpQjADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAKBDMAABQIZgAAKBDMAABQIJgBAKBAMAMAQIFgBgCAAsEMAAAFghkAAAoEMwAAFAhmAAAoEMwAAFAgmAEAoEAwAwBAgWAGAIACwQwAAAWCGQAACgQzAAAUtEzlwY4dO5YHHnggg4ODmTlzZtauXZsvf/nLU3kKAACYVlN6hflPf/pTjhw5kmeeeSb33ntvfvGLX0zl4QEAYNpNaTC/9tprufrqq5MkX/va1/LWW29N5eEBAGDaTWkwj4yMZPbs2eOPq9Vqjh49OpWnAACAaTWl9zDPnj07o6Oj44+PHTuWlpbyKTo62qZyBE4R9oJG7AWN2AsasRdMpSm9wvz1r389O3fuTJL87W9/S1dX11QeHgAApl2lXq/Xp+pgH/2WjD179qRer+fnP/955s+fP1WHBwCAaTelwQwAAKcaH1wCAAAFghkAAAoEMwAAFEzpr5X7rHyENh+p1WpZuXJl9u3blyNHjmTp0qW54IILsmLFilQqlVx44YVZs2ZNZszw3u5088EHH+Smm27K7373u7S0tNgJkiSPPvpoXnjhhdRqtSxZsiTd3d124zRXq9WyYsWK7Nu3LzNmzMiDDz7oe8Zp7I033sivfvWrbNmyJe+++27DPdi+fXu2bduWlpaWLF26NNdee+2Ex23K9vgIbT7y/PPPZ86cOdm6dWs2b96cBx98MOvWrUt/f3+2bt2aer2eHTt2NHtMplmtVsvq1atzxhlnJImdIEkyMDCQ119/PU8//XS2bNmSAwcO2A3y0ksv5ejRo9m2bVuWLVuWhx9+2F6cpjZv3pz7778/hw8fTtL4746hoaFs2bIl27Zty+OPP56NGzfmyJEjEx67KcHsI7T5yPXXX5+77757/HG1Ws3u3bvT3d2dJFm8eHF27drVrPFokvXr1+fWW2/NOeeckyR2giTJyy+/nK6urixbtix33nlnrrnmGrtBzj///IyNjeXYsWMZGRlJS0uLvThNzZs3L5s2bRp/3GgP3nzzzVx22WWZOXNm2traMm/evLzzzjsTHrspwewjtPlIa2trZs+enZGRkdx1113p7+9PvV5PpVIZf354eLjJUzKdnnvuucydO3f8TXUSO0GS5NChQ3nrrbfy61//Oj/72c/yk5/8xG6QM888M/v27csNN9yQVatWpa+vz16cpnp6ev7rE6Yb7cHIyEja2v7zKZCtra0ZGRmZ8NhNuYd5Mh+hzalr//79WbZsWXp7e3PjjTdmw4YN48+Njo6mvb29idMx3Z599tlUKpW88sorefvtt7N8+fIcPHhw/Hk7cfqaM2dOOjs7M3PmzHR2dmbWrFk5cODA+PN24/T0xBNP5Kqrrsq9996b/fv35/bbb0+tVht/3l6cvv7vfesf7cHHG3R0dPS/AvpTj3VCJpyAj9DmI++//37uuOOO3Hfffbn55puTJAsWLMjAwECSZOfOnVm4cGEzR2SaPfXUU3nyySezZcuWXHzxxVm/fn0WL15sJ8jll1+eP//5z6nX6/nnP/+ZDz/8MFdeeaXdOM21t7ePB89ZZ52Vo0eP+nuEJI174tJLL81rr72Ww4cPZ3h4OHv37v1MHdqUT/rzEdp8ZO3atfnjH/+Yzs7O8T/76U9/mrVr16ZWq6WzszNr165NtVpt4pQ0S19fXx544IHMmDEjq1atshPkl7/8ZQYGBlKv13PPPffkvPPOsxunudHR0axcuTJDQ0Op1Wq57bbbcskll9iL09R7772XH//4x9m+fXv+/ve/N9yD7du355lnnkm9Xs8Pf/jD9PT0THhcH40NAAAFfikhAAAUCGYAACgQzAAAUCCYAQCgQDADAECBYAYAgALBDAAABYIZAAAK/h9PZy/jSbc/8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds=5\n",
    "X_train,X_test,y_train,y_test=TS_crossval(X, y, k=folds, test_percent = 0.2)\n",
    "\n",
    "# or with subsamples\n",
    "# sub_sample = 100\n",
    "# X_train,X_test,y_train,y_test=TS_crossval(X[:sub_sample],y[:sub_sample],\n",
    "#                                                 k=folds, test_percent = 0.2) #Sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fa2eb",
   "metadata": {},
   "source": [
    "# Score of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6952cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(y_test, y_pred):\n",
    "    # Can be any score based on the problem\n",
    "    score = (stats.spearmanr(y_test, y_pred))[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940876c",
   "metadata": {},
   "source": [
    "# Deep Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa875b",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd4179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model(X,y,X_t,y_t,lr=0.005,bs=64, ep=10, actv='relu',\n",
    "                  min_lr=0.00005, f_lr=0.7, reg=0,h_layers=2, multitask=False, nodes = 512, drpout=0.2):\n",
    "    #X=AE_r.predict(X)\n",
    "    #X_t=AE_r.predict(X_t)\n",
    "    score={}\n",
    "    input_sz=X.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_sz,)))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(nodes*2,activation=actv, kernel_regularizer=regularizers.l1_l2(l1=reg, l2=reg),\n",
    "                    bias_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dropout(drpout))\n",
    "    for h_layer in range(int(h_layers)+1):\n",
    "        model.add(Dense(nodes,activation=actv, kernel_regularizer=regularizers.l1_l2(l1=reg, l2=reg),\n",
    "                        bias_regularizer=regularizers.l2(reg)))\n",
    "        model.add(Dropout(drpout))\n",
    "    #model.add(Dense(100,activation=actv, kernel_regularizer=regularizers.l1_l2(l1=reg, l2=reg),\n",
    "     #                   bias_regularizer=regularizers.l2(reg)))\n",
    "    #model.add(Dropout(0.3))\n",
    "    if multitask:\n",
    "        model.add(Dense(y.shape[1],activation='sigmoid'))\n",
    "    else: \n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,loss='mse')\n",
    "    ES = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=20)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=f_lr,\n",
    "                              patience=6,verbose=1, min_lr=min_lr)\n",
    "#     print(\"Epoch LR: \",K.eval(model.optimizer.lr))\n",
    "    model.hist = model.fit(X,y,epochs=ep, callbacks=[ES,reduce_lr],\n",
    "              validation_data=(X_t,y_t),shuffle=False, batch_size=bs,verbose=1)\n",
    "    if multitask:\n",
    "        \n",
    "        for i in range(y.shape[1]):\n",
    "            score[str(i)+'_'+'train_score'] = scorer(y.iloc[:,i],model.predict(X)[:,i])\n",
    "            score[str(i)+'_'+'test_score'] = scorer(y_t.iloc[:,i],model.predict(X_t)[:,i])\n",
    "    else:\n",
    "        for i in range(1):\n",
    "            score[str(i)+'_'+'train_score'] = scorer(y,model.predict(X)[:,i])\n",
    "            score[str(i)+'_'+'test_score'] = scorer(y_t,model.predict(X_t)[:,i])\n",
    "\n",
    "    return model, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1ff3e",
   "metadata": {},
   "source": [
    "# Conv ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1746834",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a151d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg_boost_r(X_train, y_train,X_test=None ,y_test=None,n_est=300,lr=0.001,max_d=2,subcols=0.5,min_leaf=3):\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', max_depth=max_d, learning_rate=lr,\n",
    "                          n_estimators=n_est, n_jobs=-1, colsample_bytree=subcols,min_child_weight=min_leaf)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    model.train_score=scorer(y_train, pred_train)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        pred_test = model.predict(X_test)\n",
    "        model.test_score=scorer(y_test, pred_test)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb9dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_r(X_train, y_train,X_test=None ,y_test=None,n_est=300,lr=0.01,max_d=2,subcols=0.5,min_leaf=31):\n",
    "    \n",
    "    model = lgb.LGBMRegressor(max_depth=max_d, learning_rate=lr,\n",
    "                          n_estimators=n_est, colsample_bytree=subcols,num_leaves=min_leaf)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    model.train_score=scorer(y_train, pred_train)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        pred_test = model.predict(X_test)\n",
    "        model.test_score=scorer(y_test, pred_test)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed50e1",
   "metadata": {},
   "source": [
    "# Random Search\n",
    "Setting \"ratio = 1\" in the function will turn this to a GRID SEARCH for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "691320aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(params,ratio=0.2):\n",
    "    p = list(params.values())\n",
    "    t = []\n",
    "    for r in itertools.product(*p): t.append([*r])\n",
    "    num = int(ratio * len(t))\n",
    "    random_locs = np.random.permutation(len(t))[:num]\n",
    "    print(num,\"sets of parameters has randomly selected among\", len(t), \"possible sets\")\n",
    "    return [t[i] for i in list(random_locs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2461fd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 sets of parameters has randomly selected among 288 possible sets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2e-05, 100, 3, 64, 400, 0.4], [0.001, 100, 6, 64, 800, 0.4]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_ann={'lr': [1e-3,1e-4,2e-5,1e-5], \n",
    "           'ep':[100,200],\n",
    "           'h_layer':[1,3,6],\n",
    "           'bs':[16,64],\n",
    "           'nodes':[100,400,800],\n",
    "           'drp' : [0.2,0.4]}\n",
    "param_cml={'subcols':[0.2,0.4,0.6,0.8], \n",
    "           'max_d':[5,8,9,10,12,15],\n",
    "           'lr': [0.1,0.05,0.01,0.005,0.001],\n",
    "           'min_l': [15,20],\n",
    "           'n_est': [90,150,300,1500]}\n",
    "random_search(param_ann,ratio=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bf9eb",
   "metadata": {},
   "source": [
    "# Tune Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4f89af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tune_model(data, param, ANN= True ,random_ratio = 0.2 ,model_name='model', folds = 5):\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_tset = data['y_test']\n",
    "    \n",
    "    if ANN:\n",
    "        random_parameters_ann = random_search(param,ratio=random_ratio)\n",
    "    else:\n",
    "        random_parameters_cml = random_search(param,ratio=random_ratio)\n",
    "    \n",
    "    Grid_cv=pd.DataFrame(columns=param.keys())\n",
    "    Grid_cv['test_score_mean']=np.nan\n",
    "    Grid_cv['test_score_std']=np.nan\n",
    "    Grid_cv['train_score_mean']=np.nan\n",
    "    Grid_cv['train_score_std']=np.nan\n",
    "    for k in range(1,folds+1):\n",
    "        Grid_cv[f'test_score_{k}']=np.nan\n",
    "    for k in range(1,folds+1):\n",
    "        Grid_cv[f'train_score_{k}']=np.nan\n",
    "\n",
    "    #####TUNE Classic ML Model ######\n",
    "    if not ANN:\n",
    "        print(len(random_parameters_cml),' models each in' , folds , 'time series cross validation ...')\n",
    "\n",
    "        i=1;\n",
    "        counter = 0\n",
    "        for subcols,max_d,lr,min_l,n_est in random_parameters_cml:\n",
    "            counter +=1\n",
    "            test_score=[]\n",
    "            train_score=[]\n",
    "            if  counter%10 == 0:\n",
    "                print('HyperParam set',counter)\n",
    "            for k in range(1,folds+1):\n",
    "              \n",
    "                  #model= xg_boost_r(X_train[k],y_train[k],X_test[k],y_test[k],\n",
    "                  #                      n_est=500,lr=lr,max_d=max_d,subcols=subcols,min_leaf=min_l)\n",
    "\n",
    "                model = lightgbm_r(X_train[k],y_train[k],X_test[k],y_test[k],\n",
    "                                        n_est=n_est,lr=lr,max_d=max_d,subcols=subcols,min_leaf=min_l)\n",
    "\n",
    "                test_score.append(model.test_score)\n",
    "                train_score.append(model.train_score)\n",
    "\n",
    "            Grid_cv.loc[i]=[subcols, max_d, lr,min_l, n_est,np.mean(test_score),np.std(test_score),\n",
    "                            np.mean(train_score),np.std(train_score),\n",
    "                           *test_score,*train_score]\n",
    "            i+=1\n",
    "        print('Done!')\n",
    "    if ANN:\n",
    "\n",
    "        print(len(random_parameters_ann),' models each in' , folds , 'time series cross validation ...')\n",
    "\n",
    "        i=1\n",
    "        counter = 0\n",
    "        for lr,ep,h_lr,bs,nodes,drp in random_parameters_ann:\n",
    "            counter +=1\n",
    "            test_score=[]\n",
    "            train_score=[]\n",
    "            for k in range(1,folds+1):\n",
    "                print(counter,'HyperParam set', 'in', k,'crossval fold')\n",
    "\n",
    "                model ,score = ANN_model(X_train[k],y_train[k],X_test[k],y_test[k],\n",
    "                                      lr=lr,bs=bs, ep=ep, actv='relu',min_lr=0.00001*lr,\n",
    "                                      f_lr=0.7, reg=0, h_layers=h_lr, nodes = nodes, drpout=drp)\n",
    "\n",
    "                test_score.append(score['0_test_score'])\n",
    "                train_score.append(score['0_train_score'])\n",
    "                \n",
    "            Grid_cv.loc[i]=[lr, ep, h_lr, bs, nodes, drp, np.mean(test_score),np.std(test_score),\n",
    "                            np.mean(train_score),np.std(train_score),\n",
    "                           *test_score,*train_score]\n",
    "            i+=1\n",
    "        print('Done!')\n",
    "        \n",
    "    return Grid_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02eb3c",
   "metadata": {},
   "source": [
    "# Let's  Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea95903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'X_train': X_train, 'X_test': X_test,\n",
    "     'y_train': y_train, 'y_test': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b11ce6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 sets of parameters has randomly selected among 48 possible sets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1e-05, 20, 3, 64, 100, 0.4],\n",
       " [0.0001, 10, 1, 64, 400, 0.4],\n",
       " [1e-05, 10, 3, 64, 400, 0.2],\n",
       " [0.001, 20, 1, 64, 100, 0.2]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_ann={'lr': [1e-3,1e-4,1e-5], \n",
    "           'ep':[10,20],\n",
    "           'h_layer':[1,3],\n",
    "           'bs':[64],\n",
    "           'nodes':[100,400],\n",
    "           'drp' : [0.2,0.4]}\n",
    "param_cml={'subcols':[0.2,0.4,0.6,0.8], \n",
    "           'max_d':[5,8,9,10,12,15],\n",
    "           'lr': [0.1,0.05,0.01,0.005,0.001],\n",
    "           'min_l': [15,20],\n",
    "           'n_est': [90,150,300,1500]}\n",
    "\n",
    "random_ratio = 0.1\n",
    "random_search(param_ann,ratio=random_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59e00b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 sets of parameters has randomly selected among 48 possible sets\n",
      "4  models each in 5 time series cross validation ...\n",
      "1 HyperParam set in 1 crossval fold\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.1276 - val_loss: 0.1306\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.1251 - val_loss: 0.1314\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.1242 - val_loss: 0.1316\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1235 - val_loss: 0.1321\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.1234 - val_loss: 0.1315\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1232 - val_loss: 0.1325\n",
      "Epoch 7/10\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.1222 - val_loss: 0.1325\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1212 - val_loss: 0.1331\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1204 - val_loss: 0.1331\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.1205 - val_loss: 0.1329\n",
      "1 HyperParam set in 2 crossval fold\n",
      "Epoch 1/10\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.1267 - val_loss: 0.1236\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1256 - val_loss: 0.1228\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1250 - val_loss: 0.1227\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1244 - val_loss: 0.1229\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.1245 - val_loss: 0.1228\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1242 - val_loss: 0.1229\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1240 - val_loss: 0.1230\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.1239 - val_loss: 0.1236\n",
      "Epoch 9/10\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1237 - val_loss: 0.1232\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1230 - val_loss: 0.1235\n",
      "1 HyperParam set in 3 crossval fold\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.1264 - val_loss: 0.1237\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1249 - val_loss: 0.1232\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1244 - val_loss: 0.1235\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1245 - val_loss: 0.1232\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1240 - val_loss: 0.1231\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1241 - val_loss: 0.1232\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1240 - val_loss: 0.1231\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1240 - val_loss: 0.1230\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1238 - val_loss: 0.1231\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.124 - 1s 10ms/step - loss: 0.1237 - val_loss: 0.1231\n",
      "1 HyperParam set in 4 crossval fold\n",
      "Epoch 1/10\n",
      "167/167 [==============================] - 2s 14ms/step - loss: 0.1262 - val_loss: 0.1238\n",
      "Epoch 2/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1250 - val_loss: 0.1234\n",
      "Epoch 3/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1247 - val_loss: 0.1231\n",
      "Epoch 4/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1243 - val_loss: 0.1232\n",
      "Epoch 5/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1243 - val_loss: 0.1227\n",
      "Epoch 6/10\n",
      "167/167 [==============================] - 2s 12ms/step - loss: 0.1243 - val_loss: 0.1227\n",
      "Epoch 7/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1240 - val_loss: 0.1226\n",
      "Epoch 8/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1237 - val_loss: 0.1226\n",
      "Epoch 9/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1238 - val_loss: 0.1229\n",
      "Epoch 10/10\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1235 - val_loss: 0.1232\n",
      "1 HyperParam set in 5 crossval fold\n",
      "Epoch 1/10\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1258 - val_loss: 0.1245\n",
      "Epoch 2/10\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1246 - val_loss: 0.1245\n",
      "Epoch 3/10\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.1241 - val_loss: 0.1243\n",
      "Epoch 4/10\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.1240 - val_loss: 0.1244\n",
      "Epoch 5/10\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1239 - val_loss: 0.1239\n",
      "Epoch 6/10\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.1237 - val_loss: 0.1237\n",
      "Epoch 7/10\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.1235 - val_loss: 0.1233\n",
      "Epoch 8/10\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1235 - val_loss: 0.1236\n",
      "Epoch 9/10\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.1233 - val_loss: 0.1238\n",
      "Epoch 10/10\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.1233 - val_loss: 0.1243\n",
      "2 HyperParam set in 1 crossval fold\n",
      "Epoch 1/20\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.1249 - val_loss: 0.1309\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1248 - val_loss: 0.1309\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1242 - val_loss: 0.1310\n",
      "Epoch 4/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1243 - val_loss: 0.1310\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.1244 - val_loss: 0.1310\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1245 - val_loss: 0.1310\n",
      "Epoch 7/20\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.9999998231651255e-06.\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1241 - val_loss: 0.1310\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.1237 - val_loss: 0.1310\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.1236 - val_loss: 0.1310\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.1239 - val_loss: 0.1310\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.1234 - val_loss: 0.1310\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.1238 - val_loss: 0.1310\n",
      "Epoch 13/20\n",
      "40/42 [===========================>..] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.899999748886329e-06.\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1238 - val_loss: 0.1310\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1237 - val_loss: 0.1310\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1240 - val_loss: 0.1310\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1238 - val_loss: 0.1310\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1228 - val_loss: 0.1310\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1233 - val_loss: 0.1310\n",
      "Epoch 19/20\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.4299996968911726e-06.\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.1232 - val_loss: 0.1311\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.1233 - val_loss: 0.1311\n",
      "2 HyperParam set in 2 crossval fold\n",
      "Epoch 1/20\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.1254 - val_loss: 0.1233\n",
      "Epoch 2/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1250 - val_loss: 0.1232\n",
      "Epoch 3/20\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.1250 - val_loss: 0.1232\n",
      "Epoch 4/20\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.1248 - val_loss: 0.1231\n",
      "Epoch 5/20\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.1246 - val_loss: 0.1230\n",
      "Epoch 6/20\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.1244 - val_loss: 0.1230\n",
      "Epoch 7/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1245 - val_loss: 0.1230\n",
      "Epoch 8/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1246 - val_loss: 0.1229\n",
      "Epoch 9/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1242 - val_loss: 0.1229\n",
      "Epoch 10/20\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.1240 - val_loss: 0.1229\n",
      "Epoch 11/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1241 - val_loss: 0.1228\n",
      "Epoch 12/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1239 - val_loss: 0.1228\n",
      "Epoch 13/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1240 - val_loss: 0.1228\n",
      "Epoch 14/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1240 - val_loss: 0.1228\n",
      "Epoch 15/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1240 - val_loss: 0.1228\n",
      "Epoch 16/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1236 - val_loss: 0.1228\n",
      "Epoch 17/20\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.1241 - val_loss: 0.1228\n",
      "Epoch 18/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1236 - val_loss: 0.1228\n",
      "Epoch 19/20\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.9999998231651255e-06.\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1236 - val_loss: 0.1228\n",
      "Epoch 20/20\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.1233 - val_loss: 0.1228\n",
      "2 HyperParam set in 3 crossval fold\n",
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.1251 - val_loss: 0.1238\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1248 - val_loss: 0.1235\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1248 - val_loss: 0.1234\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1244 - val_loss: 0.1232\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1243 - val_loss: 0.1231\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1243 - val_loss: 0.1231\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1242 - val_loss: 0.1230\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.1241 - val_loss: 0.1230\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1240 - val_loss: 0.1229\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.1237 - val_loss: 0.1229\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1238 - val_loss: 0.1229\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1236 - val_loss: 0.1229\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1236 - val_loss: 0.1229\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1235 - val_loss: 0.1229\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1237 - val_loss: 0.1229\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.1238 - val_loss: 0.1229\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1235 - val_loss: 0.1229\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1236 - val_loss: 0.1229\n",
      "Epoch 19/20\n",
      "120/125 [===========================>..] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.9999998231651255e-06.\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1234 - val_loss: 0.1229\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1233 - val_loss: 0.1229\n",
      "2 HyperParam set in 4 crossval fold\n",
      "Epoch 1/20\n",
      "167/167 [==============================] - 2s 12ms/step - loss: 0.1250 - val_loss: 0.1237\n",
      "Epoch 2/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1247 - val_loss: 0.1234\n",
      "Epoch 3/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1246 - val_loss: 0.1232\n",
      "Epoch 4/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1246 - val_loss: 0.1231\n",
      "Epoch 5/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1245 - val_loss: 0.1230\n",
      "Epoch 6/20\n",
      "167/167 [==============================] - 2s 12ms/step - loss: 0.1243 - val_loss: 0.1229\n",
      "Epoch 7/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1241 - val_loss: 0.1228\n",
      "Epoch 8/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1241 - val_loss: 0.1228\n",
      "Epoch 9/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1241 - val_loss: 0.1228\n",
      "Epoch 10/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1241 - val_loss: 0.1227\n",
      "Epoch 11/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1239 - val_loss: 0.1227\n",
      "Epoch 12/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1238 - val_loss: 0.1227\n",
      "Epoch 13/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1238 - val_loss: 0.1227\n",
      "Epoch 14/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1239 - val_loss: 0.1226\n",
      "Epoch 15/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1238 - val_loss: 0.1226\n",
      "Epoch 16/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1237 - val_loss: 0.1226\n",
      "Epoch 17/20\n",
      "167/167 [==============================] - 2s 12ms/step - loss: 0.1237 - val_loss: 0.1226\n",
      "Epoch 18/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1238 - val_loss: 0.1226\n",
      "Epoch 19/20\n",
      "167/167 [==============================] - 2s 11ms/step - loss: 0.1237 - val_loss: 0.1226\n",
      "Epoch 20/20\n",
      "167/167 [==============================] - 2s 12ms/step - loss: 0.1238 - val_loss: 0.1225\n",
      "2 HyperParam set in 5 crossval fold\n",
      "Epoch 1/20\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.1250 - val_loss: 0.1254\n",
      "Epoch 2/20\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.1247 - val_loss: 0.1250\n",
      "Epoch 3/20\n",
      "209/209 [==============================] - 3s 13ms/step - loss: 0.1243 - val_loss: 0.1246\n",
      "Epoch 4/20\n",
      "209/209 [==============================] - 3s 13ms/step - loss: 0.1242 - val_loss: 0.1244\n",
      "Epoch 5/20\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.1240 - val_loss: 0.1242\n",
      "Epoch 6/20\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.1241 - val_loss: 0.1240\n",
      "Epoch 7/20\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.1238 - val_loss: 0.1239\n",
      "Epoch 8/20\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.1239 - val_loss: 0.1238\n",
      "Epoch 9/20\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.1237 - val_loss: 0.1237\n",
      "Epoch 10/20\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1238 - val_loss: 0.1236\n",
      "Epoch 11/20\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.1237 - val_loss: 0.1235\n",
      "Epoch 12/20\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1236 - val_loss: 0.1235\n",
      "Epoch 13/20\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.1235 - val_loss: 0.1235\n",
      "Epoch 14/20\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.1235 - val_loss: 0.1234\n",
      "Epoch 15/20\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.1233 - val_loss: 0.1234\n",
      "Epoch 16/20\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1235 - val_loss: 0.1234\n",
      "Epoch 17/20\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1234 - val_loss: 0.1234\n",
      "Epoch 18/20\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.1234 - val_loss: 0.1234\n",
      "Epoch 19/20\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.1234 - val_loss: 0.1233\n",
      "Epoch 20/20\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.1232 - val_loss: 0.1233\n",
      "3 HyperParam set in 1 crossval fold\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.1246 - val_loss: 0.1308\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 0.1239 - val_loss: 0.1309\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 1s 17ms/step - loss: 0.1232 - val_loss: 0.1312\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 0.1233 - val_loss: 0.1317\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.1225 - val_loss: 0.1321\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 1s 17ms/step - loss: 0.1229 - val_loss: 0.1320\n",
      "Epoch 7/10\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 0.1225 - val_loss: 0.1322\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 1s 17ms/step - loss: 0.1217 - val_loss: 0.1328\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 1s 17ms/step - loss: 0.1222 - val_loss: 0.1328\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 0.1216 - val_loss: 0.1329\n",
      "3 HyperParam set in 2 crossval fold\n",
      "Epoch 1/10\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.1251 - val_loss: 0.1233\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.1247 - val_loss: 0.1232\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.1243 - val_loss: 0.1233\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.1240 - val_loss: 0.1233\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.1239 - val_loss: 0.1233\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.1238 - val_loss: 0.1234\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.1233 - val_loss: 0.1232\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1235 - val_loss: 0.1231\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.1235 - val_loss: 0.1233\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.1235 - val_loss: 0.1233\n",
      "3 HyperParam set in 3 crossval fold\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.1249 - val_loss: 0.1233\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.1242 - val_loss: 0.1231\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.1242 - val_loss: 0.1231\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.1239 - val_loss: 0.1231\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.1239 - val_loss: 0.1231\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.1238 - val_loss: 0.1232\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.1238 - val_loss: 0.1231\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.1237 - val_loss: 0.1231\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.1235 - val_loss: 0.1231\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.1234 - val_loss: 0.1232\n",
      "3 HyperParam set in 4 crossval fold\n",
      "Epoch 1/10\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1250 - val_loss: 0.1232\n",
      "Epoch 2/10\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1243 - val_loss: 0.1226\n",
      "Epoch 3/10\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1243 - val_loss: 0.1227\n",
      "Epoch 4/10\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1241 - val_loss: 0.1226\n",
      "Epoch 5/10\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1239 - val_loss: 0.1225\n",
      "Epoch 6/10\n",
      "167/167 [==============================] - 3s 15ms/step - loss: 0.1241 - val_loss: 0.1225\n",
      "Epoch 7/10\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1236 - val_loss: 0.1224\n",
      "Epoch 8/10\n",
      "167/167 [==============================] - 3s 15ms/step - loss: 0.1236 - val_loss: 0.1225\n",
      "Epoch 9/10\n",
      "167/167 [==============================] - 3s 15ms/step - loss: 0.1235 - val_loss: 0.1225\n",
      "Epoch 10/10\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1236 - val_loss: 0.1225\n",
      "3 HyperParam set in 5 crossval fold\n",
      "Epoch 1/10\n",
      "209/209 [==============================] - 3s 16ms/step - loss: 0.1246 - val_loss: 0.1238\n",
      "Epoch 2/10\n",
      "209/209 [==============================] - 3s 15ms/step - loss: 0.1242 - val_loss: 0.1234\n",
      "Epoch 3/10\n",
      "209/209 [==============================] - 3s 15ms/step - loss: 0.1240 - val_loss: 0.1234\n",
      "Epoch 4/10\n",
      "209/209 [==============================] - 3s 16ms/step - loss: 0.1237 - val_loss: 0.1232\n",
      "Epoch 5/10\n",
      "209/209 [==============================] - 3s 15ms/step - loss: 0.1236 - val_loss: 0.1234\n",
      "Epoch 6/10\n",
      "209/209 [==============================] - 3s 16ms/step - loss: 0.1235 - val_loss: 0.1230\n",
      "Epoch 7/10\n",
      "209/209 [==============================] - 3s 16ms/step - loss: 0.1234 - val_loss: 0.1229\n",
      "Epoch 8/10\n",
      "209/209 [==============================] - 3s 15ms/step - loss: 0.1233 - val_loss: 0.1230\n",
      "Epoch 9/10\n",
      "209/209 [==============================] - 3s 15ms/step - loss: 0.1232 - val_loss: 0.1229\n",
      "Epoch 10/10\n",
      "209/209 [==============================] - 3s 15ms/step - loss: 0.1228 - val_loss: 0.1231\n",
      "4 HyperParam set in 1 crossval fold\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.1255 - val_loss: 0.1314\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1314\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1313\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1313\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1254 - val_loss: 0.1313\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1312\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1312\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1251 - val_loss: 0.1312\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1245 - val_loss: 0.1312\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.1251 - val_loss: 0.1311\n",
      "4 HyperParam set in 2 crossval fold\n",
      "Epoch 1/10\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1273 - val_loss: 0.1242\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1263 - val_loss: 0.1240\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 0.1239\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 0.1238\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1260 - val_loss: 0.1237\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1263 - val_loss: 0.1236\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1260 - val_loss: 0.1236\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1235\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1259 - val_loss: 0.1235\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1258 - val_loss: 0.1234\n",
      "4 HyperParam set in 3 crossval fold\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.1267 - val_loss: 0.1254\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1259 - val_loss: 0.1248\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1257 - val_loss: 0.1246\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1259 - val_loss: 0.1244\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.1256 - val_loss: 0.1243\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.1253 - val_loss: 0.1242\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1242\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1250 - val_loss: 0.1241\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1253 - val_loss: 0.1241\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.1255 - val_loss: 0.1240\n",
      "4 HyperParam set in 4 crossval fold\n",
      "Epoch 1/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 0.1273 - val_loss: 0.1256\n",
      "Epoch 2/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1263 - val_loss: 0.1252\n",
      "Epoch 3/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1259 - val_loss: 0.1249\n",
      "Epoch 4/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1260 - val_loss: 0.1248\n",
      "Epoch 5/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1257 - val_loss: 0.1247\n",
      "Epoch 6/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1256 - val_loss: 0.1246\n",
      "Epoch 7/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1256 - val_loss: 0.1245\n",
      "Epoch 8/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1253 - val_loss: 0.1244\n",
      "Epoch 9/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1255 - val_loss: 0.1244\n",
      "Epoch 10/10\n",
      "167/167 [==============================] - 1s 4ms/step - loss: 0.1252 - val_loss: 0.1243\n",
      "4 HyperParam set in 5 crossval fold\n",
      "Epoch 1/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1263 - val_loss: 0.1261\n",
      "Epoch 2/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1260 - val_loss: 0.1259\n",
      "Epoch 3/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1258 - val_loss: 0.1258\n",
      "Epoch 4/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1255 - val_loss: 0.1256\n",
      "Epoch 5/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1254 - val_loss: 0.1255\n",
      "Epoch 6/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1255 - val_loss: 0.1253\n",
      "Epoch 7/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1254 - val_loss: 0.1252\n",
      "Epoch 8/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1251 - val_loss: 0.1251\n",
      "Epoch 9/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1251 - val_loss: 0.1250\n",
      "Epoch 10/10\n",
      "209/209 [==============================] - 1s 4ms/step - loss: 0.1250 - val_loss: 0.1249\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Random_cv = Tune_model(data, param = param_ann, ANN= True, random_ratio = random_ratio , model_name='ANN', folds = folds)\n",
    "# Random_cv.to_csv('tune_random_search.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b6ea4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>ep</th>\n",
       "      <th>h_layer</th>\n",
       "      <th>bs</th>\n",
       "      <th>nodes</th>\n",
       "      <th>drp</th>\n",
       "      <th>test_score_mean</th>\n",
       "      <th>test_score_std</th>\n",
       "      <th>train_score_mean</th>\n",
       "      <th>train_score_std</th>\n",
       "      <th>test_score_1</th>\n",
       "      <th>test_score_2</th>\n",
       "      <th>test_score_3</th>\n",
       "      <th>test_score_4</th>\n",
       "      <th>test_score_5</th>\n",
       "      <th>train_score_1</th>\n",
       "      <th>train_score_2</th>\n",
       "      <th>train_score_3</th>\n",
       "      <th>train_score_4</th>\n",
       "      <th>train_score_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.085989</td>\n",
       "      <td>0.037364</td>\n",
       "      <td>0.157224</td>\n",
       "      <td>0.034835</td>\n",
       "      <td>0.029873</td>\n",
       "      <td>0.061162</td>\n",
       "      <td>0.092677</td>\n",
       "      <td>0.108748</td>\n",
       "      <td>0.137488</td>\n",
       "      <td>0.216755</td>\n",
       "      <td>0.173812</td>\n",
       "      <td>0.139496</td>\n",
       "      <td>0.117324</td>\n",
       "      <td>0.138734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.096481</td>\n",
       "      <td>0.041767</td>\n",
       "      <td>0.127713</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>0.032846</td>\n",
       "      <td>0.068664</td>\n",
       "      <td>0.103415</td>\n",
       "      <td>0.126910</td>\n",
       "      <td>0.150567</td>\n",
       "      <td>0.138644</td>\n",
       "      <td>0.127212</td>\n",
       "      <td>0.123905</td>\n",
       "      <td>0.119945</td>\n",
       "      <td>0.128860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.097142</td>\n",
       "      <td>0.041214</td>\n",
       "      <td>0.151115</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>0.038706</td>\n",
       "      <td>0.071361</td>\n",
       "      <td>0.091310</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.154919</td>\n",
       "      <td>0.177604</td>\n",
       "      <td>0.153053</td>\n",
       "      <td>0.132526</td>\n",
       "      <td>0.142777</td>\n",
       "      <td>0.149616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.044498</td>\n",
       "      <td>0.047410</td>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.023009</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.016246</td>\n",
       "      <td>0.057099</td>\n",
       "      <td>0.131798</td>\n",
       "      <td>0.031275</td>\n",
       "      <td>0.019121</td>\n",
       "      <td>0.061078</td>\n",
       "      <td>0.052102</td>\n",
       "      <td>0.084874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lr    ep  h_layer    bs  nodes  drp  test_score_mean  test_score_std  \\\n",
       "1  0.00100  10.0      1.0  64.0  400.0  0.4         0.085989        0.037364   \n",
       "2  0.00001  20.0      1.0  64.0  400.0  0.2         0.096481        0.041767   \n",
       "3  0.00010  10.0      3.0  64.0  400.0  0.2         0.097142        0.041214   \n",
       "4  0.00001  10.0      1.0  64.0  100.0  0.2         0.044498        0.047410   \n",
       "\n",
       "   train_score_mean  train_score_std  test_score_1  test_score_2  \\\n",
       "1          0.157224         0.034835      0.029873      0.061162   \n",
       "2          0.127713         0.006259      0.032846      0.068664   \n",
       "3          0.151115         0.014985      0.038706      0.071361   \n",
       "4          0.049690         0.023009      0.015504      0.001842   \n",
       "\n",
       "   test_score_3  test_score_4  test_score_5  train_score_1  train_score_2  \\\n",
       "1      0.092677      0.108748      0.137488       0.216755       0.173812   \n",
       "2      0.103415      0.126910      0.150567       0.138644       0.127212   \n",
       "3      0.091310      0.129412      0.154919       0.177604       0.153053   \n",
       "4      0.016246      0.057099      0.131798       0.031275       0.019121   \n",
       "\n",
       "   train_score_3  train_score_4  train_score_5  \n",
       "1       0.139496       0.117324       0.138734  \n",
       "2       0.123905       0.119945       0.128860  \n",
       "3       0.132526       0.142777       0.149616  \n",
       "4       0.061078       0.052102       0.084874  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1606c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
